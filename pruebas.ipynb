{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LambdaFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.command.clean import clean\n",
    "from email.mime import image\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import boto3\n",
    "import urllib\n",
    "import io\n",
    "from PIL import Image, ImageDraw, ExifTags, ImageColor\n",
    "import base64\n",
    "\n",
    "\n",
    "\n",
    "def lambda_handler():\n",
    "    # TODO implement\n",
    "\n",
    "    # logger = logging.getLogger()\n",
    "    # logger.setLevel(logging.INFO)\n",
    "\n",
    "    # bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "    # key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')\n",
    "\n",
    "    bucket='analitica-rekognition-bucket'\n",
    "    key='caras.jpg'\n",
    "   \n",
    "    print('Llamada de crop')\n",
    "\n",
    "    dictionary, num, image= detect_faces(bucket,key)\n",
    "\n",
    "    print('\\n Resultado final: \\n')\n",
    "    print(dictionary)\n",
    "    print('número de personas: ' + '{0:.0f}'.format(num) + '\\n')\n",
    "\n",
    "    print('Llamada de crop')\n",
    "    listaimg=cropFace(image,dictionary,num,key)\n",
    "\n",
    "    for image in listaimg:\n",
    "\n",
    "        image_data = Image.open(io.BytesIO(image))\n",
    "        var = 'JPEG'\n",
    "        # image_data.save(r'C:\\Users\\user\\Desktop',var)\n",
    "        image_data.show()\n",
    "\n",
    "        #Se busca si la persona esta en una collection\n",
    "        #Se recibe un booleano donde true indica que si pertence y false que no pertenece\n",
    "        faceids=search_faces(image)\n",
    "\n",
    "        #Se actualiza atributo (Status) en la base de datos en DynamoDB\n",
    "        #de acuerdo al resultado de la función search_faces para cada una de las coincidencias en la collection\n",
    "        for faceid in faceids:\n",
    "            updateItemDB(faceid)\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "def detect_faces(bucket,key):\n",
    "\n",
    "    client=boto3.client('rekognition', 'us-east-1')\n",
    "\n",
    "\n",
    "    print(bucket)\n",
    "    print(key)\n",
    "    \n",
    "    response = client.detect_faces(Image={'S3Object': {'Bucket': bucket, 'Name': key}},\n",
    "        Attributes=['ALL'])\n",
    "\n",
    "    print('Detected faces for ' + key)   \n",
    "\n",
    "    for faceDetail in response['FaceDetails']:\n",
    "        print('The detected face is between ' + str(faceDetail['AgeRange']['Low']) \n",
    "            + ' and ' + str(faceDetail['AgeRange']['High']) + ' years old')\n",
    "\n",
    "        # print('Here are the other attributes:')\n",
    "        # print(json.dumps(faceDetail, indent=4, sort_keys=True))\n",
    "\n",
    "        # Access predictions for individual face details and print them\n",
    "        print(\"Gender: \" + str(faceDetail['Gender']))\n",
    "        # print(\"Smile: \" + str(faceDetail['Smile']))\n",
    "        # print(\"Eyeglasses: \" + str(faceDetail['Eyeglasses']))\n",
    "        # print(\"Emotions: \" + str(faceDetail['Emotions'][0]))\n",
    "\n",
    "\n",
    "    # Load image from S3 bucket\n",
    "    s3_connection = boto3.resource('s3')\n",
    "    s3_object = s3_connection.Object(bucket,key)\n",
    "    s3_response = s3_object.get()\n",
    "\n",
    "    \n",
    "\n",
    "    stream = io.BytesIO(s3_response['Body'].read())\n",
    "\n",
    "    print('Hasta aquí se ejecura sin PIL')\n",
    "    image=Image.open(stream)\n",
    "\n",
    "    imgWidth, imgHeight = image.size  \n",
    "\n",
    "    print('Se va a imprimir imagen')\n",
    "\n",
    "    # draw = ImageDraw.Draw(image)  \n",
    "                \n",
    "    dict={}\n",
    "\n",
    "\n",
    "    count=1\n",
    "\n",
    "    # calculate and display bounding boxes for each detected face       \n",
    "    # print('Bounding boxes for ' + key)    \n",
    "    for faceDetail in response['FaceDetails']:\n",
    "    \n",
    "\n",
    "        \n",
    "        box = faceDetail['BoundingBox']\n",
    "        left = imgWidth * box['Left']\n",
    "        top = imgHeight * box['Top']\n",
    "        width = imgWidth * box['Width']\n",
    "        height = imgHeight * box['Height']\n",
    "\n",
    "        dict['cara'+'{0:.0f}'.format(count)]=[left,top,left+width,top+height]\n",
    "                \n",
    "        count=count+1\n",
    "\n",
    "\n",
    "        print('Left: ' + '{0:.0f}'.format(left))\n",
    "        print('Top: ' + '{0:.0f}'.format(top))\n",
    "        print('Face Width: ' + \"{0:.0f}\".format(width))\n",
    "        print('Face Height: ' + \"{0:.0f}\".format(height))\n",
    "\n",
    "        points = (\n",
    "            (left,top),\n",
    "            (left + width, top),\n",
    "            (left + width, top + height),\n",
    "            (left , top + height),\n",
    "            (left, top)\n",
    "\n",
    "        )\n",
    "        # draw.line(points, fill='#00d400', width=2)\n",
    "\n",
    "    print('número de caras')\n",
    "    numCaras=len(response['FaceDetails'])\n",
    "\n",
    "\n",
    "    # image.show()\n",
    "\n",
    "    return dict, numCaras, image\n",
    "\n",
    "\n",
    "def cropFace(image,dict,numCaras,key):\n",
    "\n",
    "    client=boto3.client('s3')\n",
    "\n",
    "    key=key[0:len(key)-4]\n",
    "\n",
    "    listaimg=[]\n",
    "\n",
    "    for i in range(1,numCaras+1):\n",
    "        \n",
    "        nombre=key+\"{0:.0f}\".format(i)\n",
    "\n",
    "        dimensiones=dict['cara'+\"{0:.0f}\".format(i)]\n",
    "\n",
    "        dim=(int(dimensiones[0]),int(dimensiones[1]),int(dimensiones[2]),int(dimensiones[3]))\n",
    "        imagecrop=image.crop(dim)\n",
    "\n",
    "\n",
    "        img_byte_arr = io.BytesIO()\n",
    "\n",
    "\n",
    "        imagecrop.save(img_byte_arr, format=\"JPEG\")\n",
    "\n",
    "        # img_str = base64.b64encode(img_byte_arr.getvalue())\n",
    "\n",
    "        # imagecrop.save(img_byte_arr, format='JPEG')\n",
    "\n",
    "        image_file_size = img_byte_arr.tell()\n",
    "\n",
    "        print('Tamaño (bytes)')\n",
    "        print(image_file_size)\n",
    "\n",
    "        img_byte_arr = img_byte_arr.getvalue()\n",
    "\n",
    "        listaimg.append(img_byte_arr)\n",
    "\n",
    "    #     print(nombre+'.jpeg')\n",
    "\n",
    "    #     # client.upload_fileobj(img_str.read(), 'prueba-rekognition-analitica', nombre+'.jpeg')\n",
    "\n",
    "    #     client.put_object(\n",
    "    # Body=img_byte_arr,\n",
    "    # Bucket='prueba-rekognition-analitica',\n",
    "    # Key=nombre+'.jpeg',\n",
    "\n",
    "# )\n",
    "\n",
    "    print(len(listaimg))\n",
    "\n",
    "    return listaimg\n",
    "\n",
    "#Se define función para buscar caras en una imagen y relacionar con una collection\n",
    "#Recibe el nombre del bucket y de la imagen a analizar \n",
    "#Retorna lista con los FaceId de las coincidencias en la colección\n",
    "def search_faces(image):\n",
    "\n",
    "    #Cliente representando servicio de rekognition\n",
    "    client=boto3.client('rekognition', 'us-east-1')\n",
    "\n",
    "\n",
    "    collectionId = 'CollectionAnalitica' #Nombre de la colección\n",
    "    threshold = 80 #Umbral para similaridad entre caras\n",
    "    maxFaces = 100 #Número máximo de caras que quiere reconocer de la colección\n",
    "    \n",
    "    #Función del SDK (boto3) de python para buscar coincidencia con caras de una colección\n",
    "\n",
    "\n",
    "    response=client.search_faces_by_image(CollectionId=collectionId,\n",
    "                                    Image={'Bytes': image},\n",
    "                                    FaceMatchThreshold=threshold,\n",
    "                                     MaxFaces=maxFaces)\n",
    "\n",
    "\n",
    "    faceMatches = response['FaceMatches']\n",
    "\n",
    "    #Lista con el FaceId de la cara de coincidencia en la colección\n",
    "    listface=[]\n",
    "\n",
    "\n",
    "    for match in faceMatches:\n",
    "        print('FaceId:' + match['Face']['FaceId'])\n",
    "        print('ImageId:' + match['Face']['ImageId'])\n",
    "        print('Similarity: ' + \"{:.2f}\".format(match['Similarity']) + \"%\")\n",
    "        print('Confidence: ' + str(match['Face']['Confidence']))\n",
    "\n",
    "        #Si la similaridad entre coincidencia es mayor a 80% se agrega el FaceId a la lista listface\n",
    "        if match['Similarity'] > 80.0:\n",
    "            listface.append( match['Face']['FaceId'])\n",
    "\n",
    "    return listface\n",
    "\n",
    "\n",
    "#Se define función para actualizar un item de la base de datos de dynamodb\n",
    "#Recibe el FaceId del item a actualizar\n",
    "def updateItemDB(FaceId):\n",
    "\n",
    "    #Cliente representando servicio dynamodb\n",
    "    client = boto3.client('dynamodb')\n",
    "\n",
    "    try:\n",
    "\n",
    "        #Se actualiza la base de datos cambiando el atributo status con el valor booleano True\n",
    "        response = client.update_item(\n",
    "            TableName='rekognition-machine-analitic',\n",
    "            Key={\n",
    "                'FaceID': {\n",
    "                    'S': FaceId\n",
    "                }\n",
    "            },\n",
    "            AttributeUpdates={\n",
    "                'Status': {\n",
    "                    'Value': {\n",
    "                        'S': 'OK'\n",
    "                    },\n",
    "                    'Action': 'PUT'\n",
    "                }\n",
    "            },\n",
    "\n",
    "\n",
    "        )   \n",
    "\n",
    "        print('Actualizó DB')\n",
    "\n",
    "    except Exception as msg:\n",
    "\n",
    "        print(f\"Oops, no se pudo actualizar el item: {msg}\")\n",
    "\n",
    "lambda_handler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "notification = \"Here is the SNS notification for Lambda function tutorial.\"\n",
    "client = boto3.client('sns')\n",
    "\n",
    "response = client.publish (\n",
    "    TargetArn = \"arn:aws:sns:us-east-1:533886999211:SNSPrueba\",\n",
    "    Message = json.dumps({'default': notification}),\n",
    "    MessageStructure = 'json'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "client = boto3.client('sns')\n",
    "\n",
    "snsArn = 'arn:aws:sns:us-east-1:533886999211:FaceSNS'\n",
    "message = \"This is a test notification.\"\n",
    "\n",
    "response = client.publish(\n",
    "    TopicArn = snsArn,\n",
    "    Message = message ,\n",
    "    Subject='Hello'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.get_item(\n",
    "    TableName='string',\n",
    "    Key={\n",
    "        'string': {\n",
    "            'S': 'string',\n",
    "            'N': 'string',\n",
    "            'B': b'bytes',\n",
    "            'SS': [\n",
    "                'string',\n",
    "            ],\n",
    "            'NS': [\n",
    "                'string',\n",
    "            ],\n",
    "            'BS': [\n",
    "                b'bytes',\n",
    "            ],\n",
    "            'M': {\n",
    "                'string': {'... recursive ...'}\n",
    "            },\n",
    "            'L': [\n",
    "                {'... recursive ...'},\n",
    "            ],\n",
    "            'NULL': True|False,\n",
    "            'BOOL': True|False\n",
    "        }\n",
    "    },\n",
    "    AttributesToGet=[\n",
    "        'string',\n",
    "    ],\n",
    "    ConsistentRead=True|False,\n",
    "    ReturnConsumedCapacity='INDEXES'|'TOTAL'|'NONE',\n",
    "    ProjectionExpression='string',\n",
    "    ExpressionAttributeNames={\n",
    "        'string': 'string'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920.0\n",
      "1080.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture(\"rtsp://192.168.1.16:554/live1s1.sdp\",cv2.CAP_FFMPEG)\n",
    "\n",
    " #Tamaño de la imagen transmitida\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH )\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(width)\n",
    "print(height)\n",
    "\n",
    "#Se ajusta tamaño de la recepción de acuerdo al tamaño de\n",
    "#la imagen del video transmitido.\n",
    "cap.set(3, width)\n",
    "cap.set(4, height)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print('Cannot open RTSP stream')\n",
    "    exit(-1)\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    cv2.imshow('RTSP stream', frame)\n",
    "\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leyo bien\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "with open(\"/home/analitica2/analitica2/20220921_155031M.jpg\", \"rb\") as img_file:\n",
    "    if img_file.read():\n",
    "        print(\"leyo bien\")\n",
    "        print((base64.b64encode(img_file.read()).decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [x] Sent '{\"path\": \"/home/analitica2/Documentos/ftp/20220922_073004M.jpg\"}'\n",
      "Se envio imagen\n"
     ]
    }
   ],
   "source": [
    "import pika\n",
    "import json\n",
    "#método para públicar\n",
    "def publish(message, queue, mqHost):\n",
    "\n",
    "    #Se establece conexión con el servidor\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters(mqHost))\n",
    "\n",
    "    #Se comienza el canal de comunicación\n",
    "    channel = connection.channel()\n",
    "\n",
    "    #Se establece la cola\n",
    "    channel.queue_declare(queue, passive=False, durable=False, exclusive=False, auto_delete=False, arguments=None)\n",
    "\n",
    "    #Se envía mensaje\n",
    "    channel.basic_publish(\"\", queue, bytes(message, 'utf-8'), properties=None, mandatory=False)\n",
    "    print(\" [x] Sent %r\" % message)\n",
    "\n",
    "    #Se cierra la conexión\n",
    "    connection.close()\n",
    "\n",
    "data = {}\n",
    "            \n",
    "data[\"path\"] = \"/home/analitica2/Documentos/ftp/20220922_073004M.jpg\"\n",
    "mqHost = \"localhost\"   \n",
    "\n",
    "#Se envía mensaje a la cola del servicio de mensajería\n",
    "publish(json.dumps(data), \"captured-image-queue\", mqHost)\n",
    "\n",
    "print(\"Se envio imagen\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umbral Área de Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected faces for 2022101394333138015.jpg\n",
      "The detected face is between 43 and 51 years old\n",
      "Left: 471\n",
      "Top: 260\n",
      "Face Width: 77\n",
      "Face Height: 106\n",
      "AREA: 8194.353211380076\n",
      "faces detected: 1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import io\n",
    "from PIL import Image, ImageDraw, ExifTags, ImageColor\n",
    "\n",
    "def show_faces(photo,bucket):\n",
    "     \n",
    "\n",
    "    client=boto3.client('rekognition')\n",
    "\n",
    "    # Load image from S3 bucket\n",
    "    s3_connection = boto3.resource('s3')\n",
    "    s3_object = s3_connection.Object(bucket,photo)\n",
    "    s3_response = s3_object.get()\n",
    "\n",
    "    stream = io.BytesIO(s3_response['Body'].read())\n",
    "    image=Image.open(stream)\n",
    "    \n",
    "    # for orientation in ExifTags.TAGS.keys():\n",
    "    #     if ExifTags.TAGS[orientation]=='Orientation':\n",
    "    #         break\n",
    "    \n",
    "    # exif = image._getexif()\n",
    "\n",
    "    # if exif[orientation] == 3:\n",
    "    #     image=image.rotate(180, expand=True)\n",
    "    # elif exif[orientation] == 6:\n",
    "    #     image=image.rotate(270, expand=True)\n",
    "    # elif exif[orientation] == 8:\n",
    "    #     image=image.rotate(90, expand=True)\n",
    "\n",
    "    #Call DetectFaces \n",
    "    response = client.detect_faces(Image={'S3Object': {'Bucket': bucket, 'Name': photo}},\n",
    "        Attributes=['ALL'])\n",
    "\n",
    "    imgWidth, imgHeight = image.size  \n",
    "    draw = ImageDraw.Draw(image)  \n",
    "                    \n",
    "\n",
    "    # calculate and display bounding boxes for each detected face       \n",
    "    print('Detected faces for ' + photo)    \n",
    "    for faceDetail in response['FaceDetails']:\n",
    "        print('The detected face is between ' + str(faceDetail['AgeRange']['Low']) \n",
    "              + ' and ' + str(faceDetail['AgeRange']['High']) + ' years old')\n",
    "        \n",
    "        box = faceDetail['BoundingBox']\n",
    "        left = imgWidth * box['Left']\n",
    "        top = imgHeight * box['Top']\n",
    "        width = imgWidth * 1 * box['Width']\n",
    "        height = imgHeight * 1 * box['Height']\n",
    "                \n",
    "\n",
    "        print('Left: ' + '{0:.0f}'.format(left))\n",
    "        print('Top: ' + '{0:.0f}'.format(top))\n",
    "        print('Face Width: ' + \"{0:.0f}\".format(width))\n",
    "        print('Face Height: ' + \"{0:.0f}\".format(height))\n",
    "\n",
    "        points = (\n",
    "            (left,top),\n",
    "            (left + width, top),\n",
    "            (left + width, top + height),\n",
    "            (left , top + height),\n",
    "            (left, top)\n",
    "\n",
    "        )\n",
    "\n",
    "        area = width*height\n",
    "        print(\"AREA: \" +str(area))\n",
    "\n",
    "        if area > 8000:\n",
    "            draw.line(points, fill='#00d400', width=2)\n",
    "\n",
    "        # Alternatively can draw rectangle. However you can't set line width.\n",
    "        #draw.rectangle([left,top, left + width, top + height], outline='#00d400') \n",
    "\n",
    "    image.show()\n",
    "\n",
    "    return len(response['FaceDetails'])\n",
    "\n",
    "def main():\n",
    "    # bucket=\"bucket-captures\"\n",
    "    # photo=\"em.jpg\"\n",
    "\n",
    "    bucket=\"bucket-captures\"\n",
    "    photo=\"2022101394333138015.jpg\"\n",
    "\n",
    "\n",
    "\n",
    "    faces_count=show_faces(photo,bucket)\n",
    "    print(\"faces detected: \" + str(faces_count))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidS3ObjectException",
     "evalue": "An error occurred (InvalidS3ObjectException) when calling the DetectFaces operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidS3ObjectException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Documents\\Proyecto RK\\Rekognition\\pruebas.ipynb Celda 13\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/Proyecto%20RK/Rekognition/pruebas.ipynb#X20sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m# print(\"Faces detected: \" + str(face_count))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/Proyecto%20RK/Rekognition/pruebas.ipynb#X20sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/Proyecto%20RK/Rekognition/pruebas.ipynb#X20sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     main()\n",
      "\u001b[1;32mc:\\Users\\user\\Documents\\Proyecto RK\\Rekognition\\pruebas.ipynb Celda 13\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/Proyecto%20RK/Rekognition/pruebas.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m photo\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m2022927133955183669.jpg\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/Proyecto%20RK/Rekognition/pruebas.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m bucket\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mprueba-rekognition-analitica\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/Proyecto%20RK/Rekognition/pruebas.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m face_count\u001b[39m=\u001b[39mdetect_faces(photo, bucket)\n",
      "\u001b[1;32mc:\\Users\\user\\Documents\\Proyecto RK\\Rekognition\\pruebas.ipynb Celda 13\u001b[0m in \u001b[0;36mdetect_faces\u001b[1;34m(photo, bucket)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/Proyecto%20RK/Rekognition/pruebas.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m client\u001b[39m=\u001b[39mboto3\u001b[39m.\u001b[39mclient(\u001b[39m'\u001b[39m\u001b[39mrekognition\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/Proyecto%20RK/Rekognition/pruebas.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#Activar detec-faces\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/Proyecto%20RK/Rekognition/pruebas.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mdetect_faces(Image\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mS3Object\u001b[39;49m\u001b[39m'\u001b[39;49m:{\u001b[39m'\u001b[39;49m\u001b[39mBucket\u001b[39;49m\u001b[39m'\u001b[39;49m:bucket,\u001b[39m'\u001b[39;49m\u001b[39mName\u001b[39;49m\u001b[39m'\u001b[39;49m:photo}},Attributes\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mALL\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/Proyecto%20RK/Rekognition/pruebas.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# print('Detected faces for ' + photo)    \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/Proyecto%20RK/Rekognition/pruebas.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m faceDetail \u001b[39min\u001b[39;00m response[\u001b[39m'\u001b[39m\u001b[39mFaceDetails\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/Proyecto%20RK/Rekognition/pruebas.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#     print('The detected face is between ' + str(faceDetail['AgeRange']['Low']) \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/Proyecto%20RK/Rekognition/pruebas.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#           + ' and ' + str(faceDetail['AgeRange']['High']) + ' years old')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/Proyecto%20RK/Rekognition/pruebas.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/Proyecto%20RK/Rekognition/pruebas.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# print('Here are the other attributes:')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/Proyecto%20RK/Rekognition/pruebas.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# print(json.dumps(faceDetail, indent=4, sort_keys=True))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\botocore\\client.py:401\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    399\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m py_operation_name)\n\u001b[0;32m    400\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 401\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\botocore\\client.py:731\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m    729\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    730\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[1;32m--> 731\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[0;32m    732\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    733\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[1;31mInvalidS3ObjectException\u001b[0m: An error occurred (InvalidS3ObjectException) when calling the DetectFaces operation: Unable to get object metadata from S3. Check object key, region and/or access permissions."
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "def detect_faces(photo, bucket):\n",
    "\n",
    "    #Activar rekognition\n",
    "    client=boto3.client('rekognition')\n",
    "\n",
    "    #Activar detec-faces\n",
    "    response = client.detect_faces(Image={'S3Object':{'Bucket':bucket,'Name':photo}},Attributes=['ALL'])\n",
    "\n",
    "    # print('Detected faces for ' + photo)    \n",
    "    for faceDetail in response['FaceDetails']:\n",
    "    #     print('The detected face is between ' + str(faceDetail['AgeRange']['Low']) \n",
    "    #           + ' and ' + str(faceDetail['AgeRange']['High']) + ' years old')\n",
    "\n",
    "        # print('Here are the other attributes:')\n",
    "        # print(json.dumps(faceDetail, indent=4, sort_keys=True))\n",
    "        print(str(faceDetail[\"Gender\"]))\n",
    "        print(str(faceDetail[\"Pose\"]))\n",
    "\n",
    "\t\t# # Access predictions for individual face details and print them\n",
    "        # print(\"Gender: \" + str(faceDetail['Gender']))\n",
    "        # print(\"Smile: \" + str(faceDetail['Smile']))\n",
    "        # print(\"Eyeglasses: \" + str(faceDetail['Eyeglasses']))\n",
    "        # print(\"Emotions: \" + str(faceDetail['Emotions'][0]))\n",
    "\n",
    "    # return len(response['FaceDetails'])\n",
    "def main():\n",
    "    photo='2022927133955183669.jpg'\n",
    "    bucket='prueba-rekognition-analitica'\n",
    "    face_count=detect_faces(photo, bucket)\n",
    "    # print(\"Faces detected: \" + str(face_count))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FACENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from facenet_pytorch import MTCNN\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "# Lectura de imágenes\n",
    "# ==============================================================================\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#imagen_1 = Image.open('/home/analitica2/Documentos/borrar/20220922_134554.jpg')\n",
    "#imagen_2 = Image.open('/home/analitica2/Documentos/borrar/20220922_134552.jpg')\n",
    "\n",
    "imagen_1 = cv2.imread('C:/Users/user/Pictures/natalia.jpg')\n",
    "imagen_1 = cv2.cvtColor(imagen_1,cv2.COLOR_BGR2RGB)\n",
    "imagen_2 = cv2.imread('C:/Users/user/Pictures/telemetrik.jpeg')\n",
    "imagen_2 = cv2.cvtColor(imagen_2,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# # Representación de imágenes\n",
    "# # ==============================================================================\n",
    "# plt.figure(figsize=(5, 4))\n",
    "# plt.imshow(imagen_1)\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.imshow(imagen_2)\n",
    "# plt.axis('off')\n",
    "# Detectar si se dispone de GPU cuda\n",
    "# ==============================================================================\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# print('Running on device: {}'.format(device))\n",
    "\n",
    "# Detector MTCNN\n",
    "# ==============================================================================\n",
    "mtcnn = MTCNN(\n",
    "            select_largest = True,\n",
    "            min_face_size  = 15,\n",
    "            thresholds     = [0.8, 0.75, 0.75],\n",
    "            post_process   = False,\n",
    "            image_size     = 160,\n",
    "            # device         = device\n",
    "        )\n",
    "# Detección de bounding box y landmarks\n",
    "# ==============================================================================\n",
    "boxes, probs, landmarks = mtcnn.detect(imagen_1, landmarks=True)\n",
    "print('Bounding boxes:', boxes)\n",
    "print('Probability:', probs)\n",
    "print('landmarks:', landmarks)\n",
    "\n",
    "\n",
    "\n",
    "# Representación con matplotlib\n",
    "# ==============================================================================\n",
    "# En punto de origen (0,0) de una imagen es la esquina superior izquierda\n",
    "box = boxes[0]\n",
    "landmark = landmarks[0]\n",
    "fig, ax  = plt.subplots(figsize=(5, 4))\n",
    "ax.imshow(imagen_1)\n",
    "ax.scatter(landmark[:, 0], landmark[:, 1], s=8, c= 'red')\n",
    "rect = plt.Rectangle(\n",
    "            xy     = (box[0], box[1]),\n",
    "            width  = box[2] - box[0],\n",
    "            height = box[3] - box[1],\n",
    "            fill   = False,\n",
    "            color  = 'red'\n",
    "       )\n",
    "ax.add_patch(rect)\n",
    "ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#pathRecuadro = \"/home/analitica2/Documentos/RecuadrosPersonas/\"+\"12.jpg\"\n",
    "        \n",
    "#cv2.imwrite(pathRecuadro, imagen_1)\n",
    "\n",
    "# Detección de bounding box y landmarks\n",
    "# ==============================================================================\n",
    "boxes, probs, landmarks = mtcnn.detect(imagen_2, landmarks=True)\n",
    "\n",
    "try:\n",
    "    # Representación con matplotlib\n",
    "    # ==============================================================================\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    ax.imshow(imagen_2)\n",
    "\n",
    "    for box, landmark in zip(boxes, landmarks):\n",
    "        ax.scatter(landmark[:, 0], landmark[:, 1], s=8, c= 'red')\n",
    "        rect = plt.Rectangle(\n",
    "                    xy     = (box[0], box[1]),\n",
    "                    width  = box[2] - box[0],\n",
    "                    height = box[3] - box[1],\n",
    "                    fill   = False,\n",
    "                    color  = 'red'\n",
    "            )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "except:\n",
    "    print(\"No se encontraron caras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
